{"./":{"url":"./","title":"About","keywords":"","body":"READ ME © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"tags.html":{"url":"tags.html","title":"tags","keywords":"","body":"Tags © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/mininet 簡介.html":{"url":"md/mininet/mininet 簡介.html","title":"mininet 簡介","keywords":"","body":"mininet 簡介 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/mininet 簡介/什麼是 mininet.html":{"url":"md/mininet/mininet 簡介/什麼是 mininet.html","title":"什麼是 mininet","keywords":"","body":"什麼是 mininet mininet是一個網路模擬器(network emulator) 或者，更精確的說：網路拓樸模擬器(network emulation orchestration system) 它模擬了一整個終端主機(end-hosts)、路由器(router)、交換器(switches)集成系統 可以輕易的製作支援SDN的區域網路 Mininet作為一個輕量級的SDN仿真工具 它只是跑在你的你電腦的一個process mininet模擬的host行為跟真的一樣：你可以SSH(Secure Shell)進去並運行任意程式 你也可以送出封包，就像透過真的乙太網路出去，有link speed & delay 通常透過mininet模擬的架構,會與實際透過硬體架設的結果一致 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/軟體定義網路（SDN）.html":{"url":"md/mininet/軟體定義網路（SDN）.html","title":"軟體定義網路（SDN）","keywords":"","body":"軟體定義網路（SDN） © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/軟體定義網路（SDN）/什麼是 SDN.html":{"url":"md/mininet/軟體定義網路（SDN）/什麼是 SDN.html","title":"什麼是 SDN","keywords":"","body":"什麼是 SDN 軟體定義式網路（Software defined Networking，SDN） 現今的網路架構是建立於擴展樹協定（Spanning Tree Protocol，STP）上的三層式架構，透過各種傳輸協定來傳送封包， 然而，隨著雲端應用服務及巨量資料需求日益增加，網際網路的路由表越來越複雜，讓目前的網路架構產生了許多問題，越來越不敷使用。 為了要實現各種網路協定， 交換器或是路由器必須不斷的拆分及重組封包， 導致傳輸效率不佳， 無法有效發揮網路頻寬； 網路管理人員需要客製調整各種網路設定時，必須針對每臺交換器或路由器，逐一登入命令執行介面（command-line interface，CLI）設定，相當麻煩， 也不易快速變動網路架構來因應企業建置新系統的需求。 而且透過人工逐一設定的方式也有很高的風險，一旦網路管理人員輸入了錯誤的指令，很容易造成網路服務癱瘓。 網通廠商的設備雖然能通過共通的協定進行傳輸， 但是各有各的網路管理技術或是網路作業系統軟體， 網管軟體彼此之間難以相容， 一旦企業購買某一廠牌的設備， 未來更新設備時就必須遷就於該廠牌的網管功能， 無法選用其他廠牌的設備， 造成被網通廠商挾持的情形。 透過軟體來改變網路架構與機能 SDN的特色是修改了傳統網路架構的控制模式， 將網路分為控制層（Control Plane）與資料層（Data Plane 將網路的管理權限交由控制層的控制器（Controller）軟體負責，採用集中控管的方式 網路能夠自動調配 網路資源的調度提供靈活度與彈性 協助企業或組織快速部署新的應用系統 而OpenFlow技術則是一項通訊協定 用於控制層和資料層間建立傳輸通道 就像是人類的神經一樣，負責大腦與四肢的溝通 因為傳輸路徑已預先設定完成，交換器不需要透過不斷學習來尋找封包傳送的路徑，可大幅提升傳輸效率，降低延遲（Latency）的時間。 Google、Facebook、Yahoo、微軟等多家指標型的大企業投入了SDN架構與OpenFlow技術的發展 開發SDN應用程式的門檻較高 於硬體設備廠商來說，將會是一大衝擊 未來客製化的軟體就可以提供各項硬體設備的功能 交換器的重要性將會不如以往(功能會越來越單純，未來可能僅負責封包的傳送) 臺灣企業還不了解SDN © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/軟體定義網路（SDN）/跟 OpenFlow 的關係.html":{"url":"md/mininet/軟體定義網路（SDN）/跟 OpenFlow 的關係.html","title":"跟 OpenFlow 的關係","keywords":"","body":"跟 OpenFlow 的關係 OpenFlow協定目前也是實現SDN架構最主流的技術。 實現SDN網路控制技術標準化的傳輸協定「OpenFlow」,說明控制器與交換器之間溝通方式 讓控制層能夠和轉送層互動 但它只是整體SDN架構的一部分，而且也並非唯一可用的協定 OpenFlow技術將封包傳送的路徑看成是一條「Flow」，就好像是專屬的傳輸路徑， 網管人員可依據企業政策或是服務層級協議（Service Level Agreement，SLA）在控制器軟體上設定各項網管功能以及預先建立邏輯網路，來決定封包傳輸方式 例如經過哪些交換器，需要多少的網路頻寬，再將傳輸路徑設定成OpenFlow路由表（Flow Table）。 接著在控制層和資料層之間利用SSL加密技術建立起安全的傳輸通道，控制器會將設定好的OpenFlow路由表透過傳輸通道傳送給資料層的網路設備來進行封包派送。 OpenFlow是SDN的一部分，但SDN不是只有OpenFlow Openflow.org網站來分享OpenFlow的相關軟體及技術文件 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/軟體定義網路（SDN）/為什麼要用 SDN.html":{"url":"md/mininet/軟體定義網路（SDN）/為什麼要用 SDN.html","title":"為什麼要用 SDN","keywords":"","body":"為什麼要用 SDN 可以讓網路的管理變得更集中 自動處理與動態因應變化 可以減少IT服務日常維運 讓網路的設計、部署、管理、規模延展更為容易 減少人為出錯 Google運用了OpenFlow傳輸協定來打造內部資料中心的SDN 架構，新的網路架構讓Google原本只有30～40％的網路頻寬使用率，一口氣提升了3倍，高達95%的使用率 ->可節省支出 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/mininet 實戰.html":{"url":"md/mininet/mininet 實戰.html","title":"mininet 實戰","keywords":"","body":"mininet 實戰 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/mininet 實戰/安裝.html":{"url":"md/mininet/mininet 實戰/安裝.html","title":"安裝","keywords":"","body":"安裝 sudo apt-get install mininet © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/mininet 實戰/啟動.html":{"url":"md/mininet/mininet 實戰/啟動.html","title":"啟動","keywords":"","body":"啟動 mn © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/mininet 實戰/測試.html":{"url":"md/mininet/mininet 實戰/測試.html","title":"測試","keywords":"","body":"測試 nodes 查看各個節點 net 可以看到各個鏈節訊息 dump 可看到各節點的訊息 h1 ping -c 1 h2 可以用h1 ping h2 一個封包 xterm h1 h2 叫出兩個host的命令視窗 wireshark 這邊可以透過執行mininet後，在執行ps auxww | grep mininet，應該會看到類似下面的結果 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/mininet 實戰/範例.html":{"url":"md/mininet/mininet 實戰/範例.html","title":"範例","keywords":"","body":"範例 Building mininet VM server farm load balancing mechanism using large number of “real” servers (50 servers) If you try to run a mininet script like this , maybe you will get this problem if you are under Ubuntu. Exception: Could not find a default OpenFlow controller This is trigger because your system don't find a default controller . To enable it just add the first line and add the controller argument to net variable like the second line: from mininet.node import OVSController net = Mininet(topo=topo,host=CPULimitedHost, link=TCLink,controller = OVSController) Now, you will get the following error: c0 Cannot find required executable ovs-controller. Please make sure that it is installed and available in your $PATH: (/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin) ovs-controller is a legacy name. You need to be sure that you has it. sudo apt-get install openvswitch-testcontroller sudo cp /usr/bin/ovs-testcontroller /usr/bin/ovs-controller If you run it again, you should kill ovs-testcontroller first to avoid this: Exception: Please shut down the controller which is running on port 6653: Active Internet connections (servers and established) tcp 0 0 0.0.0.0:6653 0.0.0.0:* LISTEN 30215/ovs-testcontr Mininet Python API The network you'll use in this exercise includes hosts and switches connected in a linear topology, as shown in the figure below. Topo: the base class for Mininet topologies build(): The method to override in your topology class. Constructor parameters (n) will be passed through to it automatically by Topo.init(). addSwitch(): adds a switch to a topology and returns the switch name addHost(): adds a host to a topology and returns the host name addLink(): adds a bidirectional link to a topology (and returns a link key, but this is not important). Links in Mininet are bidirectional unless noted otherwise. Mininet: main class to create and manage a network start(): starts your network pingAll(): tests connectivity by trying to have all nodes ping each other stop(): stops your network net.hosts: all the hosts in a network dumpNodeConnections(): dumps connections to/from a set of nodes. setLogLevel( 'info' | 'debug' | 'output' ): set Mininet's default output level; 'info' is recommended as it provides useful information. #!/usr/bin/python import re from mininet.net import Mininet from mininet.node import Controller from mininet.cli import CLI from mininet.link import Intf from mininet.log import setLogLevel, info, error from mininet.util import quietRun from mininet.node import OVSController def checkIntf( intf ): \"Make sure intf exists and is not configured.\" if ( ' %s:' % intf ) not in quietRun( 'ip link show' ): error( 'Error:', intf, 'does not exist!\\n' ) exit( 1 ) ips = re.findall( r'\\d+\\.\\d+\\.\\d+\\.\\d+', quietRun( 'ifconfig ' + intf ) ) if ips: error( 'Error:', intf, 'has an IP address and is probably in use!\\n' ) exit( 1 ) def myNetwork(): net = Mininet( topo=None, build=False) info( '*** Adding controller\\n' ) net.addController(name='c0',controller = OVSController) info( '*** Add switches\\n') s1 = net.addSwitch('s1') max_hosts = 50 newIntf = 'enp3s0' host_list = {} info( '*** Add hosts\\n') for i in xrange(1,max_hosts+1): host_list[i] = net.addHost('h'+str(i)) info( '*** Add links between ',host_list[i],' and s1 \\r') net.addLink(host_list[i], s1) info( '*** Checking the interface ', newIntf, '\\n' ) checkIntf( newIntf ) switch = net.switches[ 0 ] info( '*** Adding', newIntf, 'to switch', switch.name, '\\n' ) brintf = Intf( newIntf, node=switch ) info( '*** Starting network\\n') net.start() CLI(net) net.stop() if __name__ == '__main__': setLogLevel( 'info' ) myNetwork() © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/運作原理.html":{"url":"md/mininet/運作原理.html","title":"運作原理","keywords":"","body":"運作原理 process-based virtualization network namespaces features that are available in recent Linux kernels hosts are emulated as bash processes running in a network namespace so any code that would normally run on a Linux server (like a web server or client program) should run just fine within a Mininet \"Host\". Python API © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/運作原理/Network Namespace.html":{"url":"md/mininet/運作原理/Network Namespace.html","title":"Network Namespace","keywords":"","body":"Network Namespace Linux Network Namespace機制更是Mininet軟體架構的基石 其實Linux Network Namespace在OpenStack和Docker等開源項目中也廣泛應用。 Mininet如何使用網絡命名空間技術？ Mininet使用Linux Network Namespaces來創建虛擬節點，默認情況下，在仿真網絡中Mininet會為每一個host創建一個新的網絡命名空間， 同時在root Namespace（根進程命名空間）運行交換機和控制器的進程，因此這兩個進程就共享同一個網絡命名空間。 由於每個主機都有各自獨立的網絡命名空間，我們就可以進行個性化的網絡配置和網絡程序部署。 由於命名空間的虛擬技術沒有提供類似於虛擬機的持久化能力，所以在Mininet關閉時不能保存所有的配置。 Mininet使用網絡命名空間來讓不同的Host進程擁有獨立的網絡上下文。 在如下的示例中，兩個虛擬主機H1和H2連接到交換機S1，通過Bash來模擬H1和H2，交換機S1運行在Linux內核運行的root namespace 。H1和H2就擁有自己的網絡命名空間以及私有網絡接口h1-eth0和h2-eth0。 交換機S1有兩個埠s1-eth0和s1-eth1，通過veth pair與對應的主機接口相連，這樣H1和H2就可以通過S1進行通信。 s1-eth0和s1-eth1間的數據包轉發通過軟體交換機完成，它運行在root namespace並使用物理接口eth0，等待控制器的指令。 其實基於Linux Network Namespace就可以原生支持作業系統層級的虛擬化，就可以被用來進行網絡仿真。 而Mininet工具使用Python語言對網絡仿真過程所涉及的節點、拓撲、鏈路等進行了封裝抽象，便於科研人員迅速開展仿真工作。 Mininet創建的Network namespace是nameless的，所以通過ip netns list命令是查看不到的， 而通過ip netns add命令創建的namespace是帶name的，這是兩者最明顯的區別之處。 Mininet可以不再關心底層系統實現，而聚焦在上層實驗邏輯上。 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/運作原理/Node.html":{"url":"md/mininet/運作原理/Node.html","title":"Node","keywords":"","body":"Node 每個Host其實就是一個Node的物件，可以在node.py中看到此物件的定義，如下。 在node.py class Node( object ): \"\"\"A virtual network node is simply a shell in a network namespace. We communicate with it using pipes.\"\"\" portBase = 0 # Nodes always start with eth0/port0, even in OF 1.0 def __init__( self, name, inNamespace=True, **params ): \"\"\"name: name of node inNamespace: in network namespace? privateDirs: list of private directory strings or tuples params: Node parameters (see config() for details)\"\"\" # Make sure class actually works self.checkSetup() self.name = params.get( 'name', name ) self.privateDirs = params.get( 'privateDirs', [] ) self.inNamespace = params.get( 'inNamespace', inNamespace ) # Stash configuration parameters for future reference self.params = params self.intfs = {} # dict of port numbers to interfaces self.ports = {} # dict of interfaces to port numbers # replace with Port objects, eventually ? self.nameToIntf = {} # dict of interface names to Intfs # Make pylint happy ( self.shell, self.execed, self.pid, self.stdin, self.stdout, self.lastPid, self.lastCmd, self.pollOut ) = ( None, None, None, None, None, None, None, None ) self.waiting = False self.readbuf = '' # Start command interpreter shell self.startShell() self.mountPrivateDirs() # File descriptor to node mapping support # Class variables and methods inToNode = {} # mapping of input fds to nodes outToNode = {} # mapping of output fds to nodes 這邊可以看到，這邊會有一個變數inNamespace用來決定此Host是否要透過network namespaces來達到network isolation的功能，當一切變數都初始化後，就會呼叫startShell()來執行此Host。 node.py def startShell( self, mnopts=None ): \"Start a shell process for running commands\" if self.shell: error( \"%s: shell is already running\\n\" % self.name ) return # mnexec: (c)lose descriptors, (d)etach from tty, # (p)rint pid, and run in (n)amespace opts = '-cd' if mnopts is None else mnopts if self.inNamespace: opts += 'n' # bash -i: force interactive # -s: pass $* to shell, and make process easy to find in ps # prompt is set to sentinel chr( 127 ) cmd = [ 'mnexec', opts, 'env', 'PS1=' + chr( 127 ), 'bash', '--norc', '-is', 'mininet:' + self.name ] # Spawn a shell subprocess in a pseudo-tty, to disable buffering # in the subprocess and insulate it from signals (e.g. SIGINT) # received by the parent master, slave = pty.openpty() self.shell = self._popen( cmd, stdin=slave, stdout=slave, stderr=slave, close_fds=False ) self.stdin = os.fdopen( master, 'rw' ) self.stdout = self.stdin self.pid = self.shell.pid self.pollOut = select.poll() self.pollOut.register( self.stdout ) # Maintain mapping between file descriptors and nodes # This is useful for monitoring multiple nodes # using select.poll() self.outToNode[ self.stdout.fileno() ] = self self.inToNode[ self.stdin.fileno() ] = self self.execed = False self.lastCmd = None self.lastPid = None self.readbuf = '' # Wait for prompt while True: data = self.read( 1024 ) if data[ -1 ] == chr( 127 ): break self.pollOut.poll() self.waiting = False # +m: disable job control notification self.cmd( 'unset HISTFILE; stty -echo; set +m' ) 這邊可以觀察到，mininet是透過一隻叫做mnexec的程式來執行 mnexec 透過參數-n來將此process給轉換到network namespaces中 當初始化兩個Host後，系統中就會出現了兩個Host，且這兩個host都會透過namespace來達到network isolation， 理論上我們要可以透過ip netns show來看到這些namespaces，實際上卻看不到，原因如同此篇所說。 https://mailman.stanford.edu/pipermail/mininet-discuss/2014-January/003796.html 此時，我們的系統如下 創立好Host後，接下來要創立Switch Switch有很多種選擇，包含了OVSLegacyKernelSwitch、UserSwitch、OVSSwitch，IVSSwitch此四種， 一般常用的就是OVSSwitch 這四種Switch都繼承自Switch物件，而Switch物件則繼承自Node Switch OVSLegacyKernelSwitch UserSwitch OVSSwitch IVSSwitch 此時系統如下，系統中已經創立好了switch以及兩個host，這三個Node都分別透過namespace來達到了network isolation，只是彼此之間都尚未有任何Link存在。 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/運作原理/Link.html":{"url":"md/mininet/運作原理/Link.html","title":"Link","keywords":"","body":"Link 接下來，會根據拓墣的Link情況去創建對應的Iterface。首先，這邊使用到Link這個物件來表示每一條Link，每個Link實際上對應到的是兩個Node上面的Interface。 link.py class Link( object ): \"\"\"A basic link is just a veth pair. Other types of links could be tunnels, link emulators, etc..\"\"\" # pylint: disable=too-many-branches def __init__( self, node1, node2, port1=None, port2=None, intfName1=None, intfName2=None, addr1=None, addr2=None, intf=Intf, cls1=None, cls2=None, params1=None, params2=None, fast=True ): \"\"\"Create veth link to another node, making two new interfaces. node1: first node node2: second node port1: node1 port number (optional) port2: node2 port number (optional) intf: default interface class/constructor cls1, cls2: optional interface-specific constructors intfName1: node1 interface name (optional) intfName2: node2 interface name (optional) params1: parameters for interface 1 params2: parameters for interface 2\"\"\" # This is a bit awkward; it seems that having everything in # params is more orthogonal, but being able to specify # in-line arguments is more convenient! So we support both. if params1 is None: params1 = {} if params2 is None: params2 = {} # Allow passing in params1=params2 if params2 is params1: params2 = dict( params1 ) if port1 is not None: params1[ 'port' ] = port1 if port2 is not None: params2[ 'port' ] = port2 if 'port' not in params1: params1[ 'port' ] = node1.newPort() if 'port' not in params2: params2[ 'port' ] = node2.newPort() if not intfName1: intfName1 = self.intfName( node1, params1[ 'port' ] ) if not intfName2: intfName2 = self.intfName( node2, params2[ 'port' ] ) self.fast = fast if fast: params1.setdefault( 'moveIntfFn', self._ignore ) params2.setdefault( 'moveIntfFn', self._ignore ) self.makeIntfPair( intfName1, intfName2, addr1, addr2, node1, node2, deleteIntfs=False ) 這邊要觀察到的，Link物件會呼叫makeIntfPair此方法，此方法就可以將兩個Interface給串接起來 util.py def makeIntfPair( intf1, intf2, addr1=None, addr2=None, node1=None, node2=None, deleteIntfs=True, runCmd=None ): \"\"\"Make a veth pair connnecting new interfaces intf1 and intf2 intf1: name for interface 1 intf2: name for interface 2 addr1: MAC address for interface 1 (optional) addr2: MAC address for interface 2 (optional) node1: home node for interface 1 (optional) node2: home node for interface 2 (optional) deleteIntfs: delete intfs before creating them runCmd: function to run shell commands (quietRun) raises Exception on failure\"\"\" if not runCmd: runCmd = quietRun if not node1 else node1.cmd runCmd2 = quietRun if not node2 else node2.cmd if deleteIntfs: # Delete any old interfaces with the same names runCmd( 'ip link del ' + intf1 ) runCmd2( 'ip link del ' + intf2 ) # Create new pair netns = 1 if not node2 else node2.pid if addr1 is None and addr2 is None: cmdOutput = runCmd( 'ip link add name %s ' 'type veth peer name %s ' 'netns %s' % ( intf1, intf2, netns ) ) else: cmdOutput = runCmd( 'ip link add name %s ' 'address %s ' 'type veth peer name %s ' 'address %s ' 'netns %s' % ( intf1, addr1, intf2, addr2, netns ) ) if cmdOutput: raise Exception( \"Error creating interface pair (%s,%s): %s \" % ( intf1, intf2, cmdOutput ) ) 這邊可以看到，mininet實際上是透過系統中的ip link的方法將兩個interface創造一條veth的Link。 此時系統如下 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/mininet/運作原理/Interface.html":{"url":"md/mininet/運作原理/Interface.html","title":"Interface","keywords":"","body":"Interface 接下來，我們要把這些interface給綁到特定的Node身上，在Link物件初始化後段，會去初始化兩個Interface真正的物件本體， link.py class Intf( object ): \"Basic interface object that can configure itself.\" def __init__( self, name, node=None, port=None, link=None, mac=None, **params ): \"\"\"name: interface name (e.g. h1-eth0) node: owning node (where this intf most likely lives) link: parent link if we're part of a link other arguments are passed to config()\"\"\" self.node = node self.name = name self.link = link self.mac = mac self.ip, self.prefixLen = None, None # if interface is lo, we know the ip is 127.0.0.1. # This saves an ifconfig command per node if self.name == 'lo': self.ip = '127.0.0.1' # Add to node (and move ourselves if necessary ) moveIntfFn = params.pop( 'moveIntfFn', None ) if moveIntfFn: node.addIntf( self, port=port, moveIntfFn=moveIntfFn ) else: node.addIntf( self, port=port ) # Save params for future reference self.params = params self.config( **params ) 這邊要觀察的重點是每個Interface都會去呼叫node.addIntf( self, port=port )來處理 node.py def addIntf( self, intf, port=None, moveIntfFn=moveIntf ): \"\"\"Add an interface. intf: interface port: port number (optional, typically OpenFlow port number) moveIntfFn: function to move interface (optional)\"\"\" if port is None: port = self.newPort() self.intfs[ port ] = intf self.ports[ intf ] = port self.nameToIntf[ intf.name ] = intf debug( '\\n' ) debug( 'added intf %s (%d) to node %s\\n' % ( intf, port, self.name ) ) if self.inNamespace: debug( 'moving', intf, 'into namespace for', self.name, '\\n' ) moveIntfFn( intf.name, self ) 此方法最後會呼叫 moveIntf 來將該interface給處理，moveIntf則會呼叫moveIntfNoRetry將Interface給綁入到每個Node中 util.py def moveIntf( intf, dstNode, printError=True, retries=3, delaySecs=0.001 ): \"\"\"Move interface to node, retrying on failure. intf: string, interface dstNode: destination Node printError: if true, print error\"\"\" retry( retries, delaySecs, moveIntfNoRetry, intf, dstNode, printError=printError ) def moveIntfNoRetry( intf, dstNode, printError=False ): \"\"\"Move interface to node, without retrying. intf: string, interface dstNode: destination Node printError: if true, print error\"\"\" intf = str( intf ) cmd = 'ip link set %s netns %s' % ( intf, dstNode.pid ) cmdOutput = quietRun( cmd ) # If ip link set does not produce any output, then we can assume # that the link has been moved successfully. if cmdOutput: if printError: error( '*** Error: moveIntf: ' + intf + ' not successfully moved to ' + dstNode.name + ':\\n', cmdOutput ) return False return True 邊可以看到，透過的指令則是ip link set %s netns %s，會將特定的interface給塞入特定Node的namespace之中 此時，我們的系統如下 OVSSwitch透過ovs-vsctl add-port將Switch上面的Interface都給OVS控管 node.py def attach( self, intf ): \"Connect a data port\" self.vsctl( 'add-port', self, intf ) self.cmd( 'ifconfig', intf, 'up' ) self.TCReapply( intf ) def vsctl( self, *args, **kwargs ): \"Run ovs-vsctl command (or queue for later execution)\" if self.batch: cmd = ' '.join( str( arg ).strip() for arg in args ) self.commands.append( cmd ) else: return self.cmd( 'ovs-vsctl', *args, **kwargs ) © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/Namespace/簡介.html":{"url":"md/Namespace/簡介.html","title":"簡介","keywords":"","body":"簡介 什麼是Namespace？ 命名空間就是在大箱子(Kernel)裏面再裝一堆小箱子(Kernel Namespace) 為什麼要這麼做？ 因為如果我們想在箱子裏面放兩個外觀一模一樣的蘋果(Process)，到時候一定會無法區分不如就放在A箱子裡面的蘋果，叫作A蘋果；放在B箱子裡面的蘋果，叫作B蘋果 命名空間有幾種？ Linux 2.6.24版的Kernel開始,提供了6種不同類型的Namespace分別是： 程序間通信(IPC)命名空間 程序命名空間 網絡命名空間 掛載命名空間 UTS命名空間 用戶命名空間 所以有人說Namespaces是一種資源隔離方案，使得PID、Network、IPC等系統資源，不再屬於全域設定，而是某個特定的Namespace的資源。 還有其他好處嘛？ 通過Namespace技術使得用戶創建的程序能夠與系統分離得更加徹底，從而不需要使用更多的底層(硬體支援)虛擬化技術因為Namespaces是用純軟體劃分出來的概念 Namespace之間有關聯嘛？ Namespace之間的資源互相隔離、不可見的因此在作業系統層面上看，就會出現多個相同pid的進程 User要怎麼看待Namespace 在用戶層面上只能看到屬於用戶自己Namespace下的資源例如使用ps命令只能列出自己Namespace下的程序使用者角度來看,每個Namespace看上去就像一個單獨的Linux系統 Image 3.1.1 - Linux的命名空間技術架構 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/Namespace/Network namespaces.html":{"url":"md/Namespace/Network namespaces.html","title":"Network namespaces","keywords":"","body":"Network namespaces © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-06-01 00:25:04 "},"md/Namespace/Network namespaces/什麼是Network Namespace.html":{"url":"md/Namespace/Network namespaces/什麼是Network Namespace.html","title":"什麼是Network Namespace","keywords":"","body":"什麼是Network Namespace 什麼是Network namespace 虛擬化網路相關的功能 Linux近幾年特有的技術(Darwin/Windows無類似功能)(約Kernel 3.0開始才有) 是輕量級虛擬化技術的基礎（Docker,LXC,OpenVZ的根本原理） 多用在虛擬化和隔離 很少被單獨使用 虛擬化網路相關的功能,是指哪些？ 不同Network namespace內的Process,具有不同的Network資源如下： 虛擬網卡列表 IPv4和IPv6協議 Routing Table 防火牆設定 /proc/net目錄 /sys/class/net目錄 埠（socket） 有什麼特性？ 讓一個或多個Process之間搞小團體小團體內有私有網路資源，小團體間互不干擾所以多用在虛擬化和隔離 如果Network Namespace之間要互相溝通怎麼辦? 通過創建veth pair（虛擬網路設備對接口）在不同的Network namespace間創建通道不同Network Namespace因此得以共享同一個實體網路設備 傳統沒有Network namespace前是怎樣？ 一般乙太網路應用程式 Image 3.2.1.1 - 一般乙太網路應用程式 如果是ADSL/光世代的PPPoE Image 3.2.1.2 - PPPoE 如果是VPN Image 3.2.1.3 - VPN 網卡拿來當Hub用的Bridge Image 3.2.1.4 - Bridge Bridge相關指令: 需要安裝bridge-utils才能使用brctl brctl show - 顯示bridge狀況 brctl addbr/delbr - 新增/刪除bridge brctl addif - 將interface新增至bridge brctl delif - 將interface從bridge移除 有Network namespace後是怎樣？ 只有一個Network namespace的話 Image 3.2.1.5 - 一個Network namespace 複數Network namespace的話 Image 3.2.1.6 - 複數Network namespace 透過veth可以連接兩個Network namespace Image 3.2.1.7 - veth 搭配Bridge使其他Network namespace上網 Image 3.2.1.8 - Bridge+Network namespace 有什麼應用場景？ 多個Network namespace可以共享eth0和lo等實體網路設備 多個Apache伺服器Process可以在不同Network namespace的80埠上進行監聽 一個Process不能嗅探其他Network namespace的流量 一個Process不能關閉其他Network namespace的接口 © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "},"md/Namespace/Network namespaces/範例.html":{"url":"md/Namespace/Network namespaces/範例.html","title":"範例","keywords":"","body":"範例 此範例的目標是？ 通過ip這個command創建H1和H2兩個Network namespace H1擁有自己的Network namespace以及私有網路網路接口h1-eth0 H2擁有自己的Network namespace以及私有網路網路接口h2-eth0 軟體Switch S1有兩個埠s1-eth0和s1-eth1 通過veth pair與對應的主機接口相連，這樣H1和H2就可以通過S1進行通信 s1-eth0和s1-eth1間的通訊資料通過軟體Switch轉發它運行在root namespace,並使用實體接口eth0，等待控制器的指令 Image 3.2.2.1 - Example1 什麼是ip command? ip這個command是iproute2軟體包裡面的一個強大的網路配置工具它能夠替代一些傳統的網路管理工具 例如:ifconfigroute 使用權限要為root幾乎所有的Linux發行版本都支持該command 介紹ip command吧 ip netns add - 新增NetNS ip netns list - 列出現有的NetNS ip netns identify - 顯示PID所在的NetNS ip netns identify $$ - 顯示目前的NetNS ip netns exec - 在NetNS執行command ip link add type veth peer name - 建立一個veth裝置 ip link set nets - 將veth 搬到NetNS Step 1 - Create host namespaces $ sudo ip netns add h1 $ sudo ip netns add h2 $ sudo ip netns Image 3.2.2.2 - Step 1-Result Step 2 - Create switch $ sudo ovs-vsctl add-br s1 Step 3 - Create links $ sudo ip link add h1-eth0 type veth peer name s1-eth1 $ sudo ip link add h2-eth0 type veth peer name s1-eth2 $ sudo ip link show Image 3.2.2.3 - Step 3-Result Step 4 - Move host ports into namespaces $ sudo ip link set h1-eth0 netns h1 $ sudo ip link set h2-eth0 netns h2 $ sudo ip netns exec h1 ip link show $ sudo ip netns exec h2 ip link show Image 3.2.2.4 - Step 4-Result Step 5 - Connect switch ports to OVS $ sudo ovs-vsctl add-port s1 s1-eth1 $ sudo ovs-vsctl add-port s1 s1-eth2 $ sudo ovs-vsctl show Step 6 - Set up OpenFlow controller $ sudo ovs-vsctl set-controller s1 tcp:127.0.0.1 $ sudo ovs-controller ptcp: $ sudo ovs-vsctl show Image 3.2.2.5 - Step 6-Result Step 7 - Configure network $ sudo ip netns exec h1 ifconfig h1-eth0 10.1 $ sudo ip netns exec h1 ifconfig lo up $ sudo ip netns exec h2 ifconfig h2-eth0 10.2 $ sudo ip netns exec h2 ifconfig lo up $ sudo ifconfig s1-eth1 up $ sudo ifconfig s1-eth2 up Step 8 - Test network $ sudo ip netns exec h1 ping -c1 10.2 Image 3.2.2.6 - Step 8-Result Image 3.2.2.7 - Final-Result © 2017 Trashman all right reserved，powered by Gitbook修訂時間： 2017-04-13 11:32:16 "}}